# 云原生技术动态 2023 年第三十一期

## 分布式系统

**1.** **平台工程六大支柱**

https://mp.weixin.qq.com/s/r7gu3lfJ6R0ATindrf3Mqg

**摘要:** 平台工程是用来设计、构建工具链和工作流的方法，软件工程师团队在这些工具和流程的帮助下，获得自助服务的能力。这些工具和流程被称为内部开发平台，经常会被简称为平台。平台团队的目标是提高开发生产力、加快发布节奏、提高应用稳定性、降低安全及合规风险，以及降低成本。

**2.** **大模型分布式并行训练系统综述**

https://mp.weixin.qq.com/s/54CHKfivLk6qRsGjsCko4g

**摘要:** 这些大模型的参数规模给训练系统带来了新的挑战，例如内存瓶颈，糟糕的运行时效率，较高的模型开发成本。当前解决这些问题的技术，主要包括：模型并行化，跨内存层级的数据溢出技术，高效的数据表示。这个综述主要探索大模型训练系统技术领域，识别关键的技术挑战，以及各种对应的解决这些挑战的技术。

## 云计算技术

**1.** **智算网络新技术追踪之一- LLR链路级重传**

https://mp.weixin.qq.com/s/yTXB6ra70ntgwgdCujWMHw

**摘要:** LLR技术不仅PCIE/CXL/NVLink等总线技术有它，Mellanox在FDR之后的IB交换机中也增加了私有化的LLR实现，直到今年UEC在技术白皮书中也明确了后期基于以太网将增加LLR技术的支持，LLR可能会成为下一代以太网的关键技术之一。

**2.** **用于训练拥有数十亿参数的大型语言模型的优化网络架构**

https://mp.weixin.qq.com/s/9zvxvJMotjzTiVVmKk2jtA

**摘要:** 本文对用于训练大型语言模型的GPU集群的传统任意连接网络架构提出了挑战。我们提出了一种新的架构，称为rail-only，它与LLM的独特特点和需求相吻合，从而实现了高达75%的网络成本降低，同时保持了与当前最先进的Clos网络相同的性能。

## 大模型技术

**1.** **NVIDIA Hopper架构对LLM系统影响分析**

https://mp.weixin.qq.com/s/ZxkpA6kjby9Z0TFarid3Hw

**摘要:** 本文假设给定的硬件架构下，对原有的LLM负载和系统，应该**最大化利用**其哪些新特性进一步加速，进而替换原有负载中的编程原语或取舍使用的优化手段。

**2.** **S-LoRA：一个GPU运行数千大模型成为可能**

https://mp.weixin.qq.com/s/obcrdbf1kpYRDTd5Qqa5sA

**摘要:** S-LoRA 提出了「统一分页」（Unified Paging）技术，即使用统一的内存池来管理不同等级的动态适配器权重和不同序列长度的 KV 缓存张量。此外，S-LoRA 还采用了新的张量并行策略和高度优化的定制 CUDA 内核，以实现 LoRA 计算的异构批处理。

## 技术之外

**1.** **大模型和深度学习的硬件设计：NVIDIA首席科学家Bill Dally精彩讲座**

https://mp.weixin.qq.com/s/IPeiapVJtljghnWSk36_kw

**摘要:** 这是非常难得的全面探讨大模型训练所依赖硬件的设计发展的讲座。作为NVIDIA高级副总裁和首席科学家，Bill Dally对当下GPT等AI大模型依赖的硬件基础给出了深入而精彩的讲解。

**2.** **AWS re:Invent 2023 - CEO Adam Selipsky 主题演讲**

https://mp.weixin.qq.com/s/Nn1rgxtLsFKhMSXS2WgV0Q

**摘要:** aws reinvent 2023 的主题演讲, 包含基础设施, 大模型 AI, 存储和数据库等多个重量级产品的发布和合作伙伴的分享.

