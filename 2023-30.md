# 云原生技术动态 2023 年第三十期

## 分布式系统

**1.** **大语言模型框架-Megatron-LM源码分析**

https://mp.weixin.qq.com/s/GFk-7Oa4sFcQcocFbcjWtg

**摘要:** Megatron核心解决问题就是提供多种分布式切分并行策略，让大语言模型能够部署在多卡分布式环境下。本文将针对，张量并行，流水并行，数据并行的实现展开源码分析。

**2.** **网络和计算的本质**

https://mp.weixin.qq.com/s/UHczh-NrT5CHsMHn75vSyQ

**摘要:** 网络的本质是承载数据流，内存是数据流在某个时刻的快照，而计算是基于快照信息而产生新的数据流。对于未来，以应用为中心,并兼顾软硬件一体化的传输协议才是我们最迫切需求。

## 云计算技术

**1.** **GPU 关键指标汇总：算力、显存、通信**

https://mp.weixin.qq.com/s/KbYKAnZYQfLB2VkKQPhCVQ

**摘要:** 理解 GPU 的规格对于分析不同模型和不同型号 GPU 间的配合非常重要, 这篇文章从算力, 显存和互联三个方面, 详细解释了影响 GPU 规格的核心原理.

**2.** **再谈谷歌Falcon以太网硬件传输协议**

https://mp.weixin.qq.com/s/cqsezkOfUgLDUGDdoX1mfQ

**摘要:** 在OCP 全球峰会上，谷歌正式宣布开放其硬件传输协议Falcon。本文对其在 OCP 峰会上的演讲视频进行了整理，帮助大家了解谷歌如何利用 Falcon 实现以太网现代化。

## 大模型技术

**1.** **Megatron源码解读3：分布式混合精度训练**

https://mp.weixin.qq.com/s/4QSYtWjZktyMU4tSsc0jVw

**摘要:** 混合精度训练的主要目的之一是为了节省显存消耗。所以在开启混合精度训练的介绍前，需要了解两个问题：（1）训练过程中，模型的哪些部分产生了显存消耗？（2）具体消耗的显存大小，要如何计算？

**2.** **可复现的语言大模型推理性能指标**

https://mp.weixin.qq.com/s/z1o_3MCiIAlwf8Rdn4EbcQ

**摘要:** LLMPerf是一个开源项目，旨在帮助用户对语言模型进行基准测试，并使其性能具有可复现性。它能够帮助用户评估不同LLM的性能，并根据具体任务做出明智的决策。该项目选择了多个指标来衡量LLM的性能，包括吞吐量、时延、内存使用和成本等。

## 技术之外

**1.** **KubeCon Chicago 主要收获：AI 的缓慢崛起，平台工程的主导地位，以及对开发者体验的重新关注**

https://mp.weixin.qq.com/s/WS_tH59w0cupP6Fli1no6Q

**摘要:** 本文讨论了 KubeCon 的主要议题，包括平台工程、Kubernetes 的不断发展、开发者体验的重要性、对应用开发和集成的关注、云原生通信的捆绑问题、安全问题的重要性、对可持续性的关注，以及社区的力量。文章强调了在标准化和创新之间取得平衡的重要性，并预测云原生的未来将看到更多的 AI/LLMs。

**2.** **稳定性，难的不是技术，而是**

https://mp.weixin.qq.com/s/9rAhbG6lu-flNIGQEF5w0g

**摘要:** 只有把稳定性当成业务的功能实现一样，有相应的人员配备和投入，以及有这样的投入的情况下，怎么去评判相应职责的团队也仍然是个很复杂的话题。
