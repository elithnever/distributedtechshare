# 云原生技术动态 2023 年第二十八期

## 分布式系统

**1.** **[RDMA可扩展性01]"真的扩展了吗？" "如扩!"**

https://mp.weixin.qq.com/s/phN75W1yvRr4S6HBCQdcqA

**摘要:** 这篇文章讨论了RDMA可扩展性问题，共两篇论文：ScaleRPC和SRNIC，分别从软件层面和硬件架构层面对RDMA可扩展性问题进行分析和优化。

**2.** **语言大模型推理性能工程：最佳实践**

https://mp.weixin.qq.com/s/mniKrBWkDE1tWWb2wQBDCA

**摘要:** 在这篇文章中，MosaicML工程师团队分享了如何在生产环境中充分利用流行开源语言大模型（LLM）的最佳实践。此外，他们还提供了围绕模型部署推理服务的指南，以帮助用户更好地选择模型和部署硬件。

## 云计算技术

**1.** **Meta的Serverless平台挺强的**

https://mp.weixin.qq.com/s/M2fN81frQLlJpBWq-lWQ_Q

**摘要:** XFaaS 是 Meta 为内部使用而开发的函数即服务（FaaS）系统，它与像 AWS Lambda、Google Cloud Functions 和 Azure Functions 这样的 FaaS 平台有很多相似之处。这篇文章结合 meta 在今年 sosp 上发表的论文, 介绍 xfaas 的设计思想.

**2.** **服务感知的零信任容器网络及其向DPU的卸载**

https://mp.weixin.qq.com/s/2K0Mta9wpaM1lF-ae1hWhg

**摘要:** 在云原生环境中，所有微服务都保持在一个扁平网络中，这带来了重大的安全问题。基于服务加密的零信任被认为是解决这个问题的主要方案。通过将 mTLS、透明 IPsec 和 OVS 卸载到 DPU 上，并重新设计工作流程以避免 DPU 的限制，实现了一个零注入和服务感知的零信任容器网络，具有线速性能。

## 云开发技术

**1.** **大模型推理加速：看图学KV Cache**

https://mp.weixin.qq.com/s/ciICyGauPtati9MITOB5Iw

**摘要:** KV Cache是Transformer标配的推理加速功能，transformer官方use_cache这个参数默认是True，但是它**只能用于Decoder架构的模型**，这是因为Decoder有因果 mask，在推理的时候前面已经生成的字符不需要与后面的字符产生attention，从而使得前面已经计算的K和V可以缓存起来。

**2.** **LLM推理技术之StreamingLLM：如何拥有无限长生成能力**

https://mp.weixin.qq.com/s/JM9jdG0t33rFSDm0br7zUQ

**摘要:** 这篇文章介绍的方法既没有扩大LLMs的上下文窗口，也没有增强它们的长期记忆。StreamingLLM的优势在于，它可以从最近的tokens生成流畅的文本，而不需要刷新缓存。

## 技术之外

**1.** **万字长文：技术之路 — 学习，成长与AI**

https://mp.weixin.qq.com/s/rR9Cra3nkguN7Iyeg8w3ow

**摘要:** 人生是一场没有标准答案的开卷考试, 这份答卷是我们自己做的，不是任何一个赞美你或者是指责你的人去做的.

**2.** **云原生和DPU：释放芯片设计的云原生潜能**

https://mp.weixin.qq.com/s/fGc59ktvXww-8Lz-g7euVQ

**摘要:** 云原生，作为一项引领当今技术潮流的重要理念，强调了可伸缩性、灵活性以及快速应用程序开发，这对芯片设计带来了深远的影响，以满足容器化和虚拟化应用程序的独特需求。在这一变革的过程中，我们不得不正视I/O延迟和内存波动等方面所带来的挑战。

