# 云计算技术动态 2024 年第十七期

## 分布式系统

**1.** **【论文解读】BypassD (ASPLOS'24)：妙用 IOMMU 高速共享访问固态盘**

https://mp.weixin.qq.com/s/RjXyJ9Ci4dnDQgtJTSOwaA

**摘要:** BypassD是一个充满想象力、实施扎实的研究工作，尽管存在限制，但为低延迟用户空间访问共享存储设备提供了新的思路。

**2.** **NSDI23 Shepherd：吞吐量提升18倍，利用率提升1.8倍**

https://mp.weixin.qq.com/s/xCIl8rD9HgmzbcKohiI_rQ

**摘要:** 本文介绍了SHEPHERD，一个分布式DNN模型服务系统。SHEPHERD采用了一个周期性规划器，将请求流聚合成中等大小的组以实现高利用率和可扩展性**，**以及一个在线调度器，它采用了一种新颖的在线算法来提供保证的吞吐量。**在生产工作负载上的评估显示，SHEPHERD的吞吐量比先前的方法高出17.2倍，利用率高出1.8倍，并且可以扩展到数百个工作节点。

## 云计算技术

**1.** **为什么GPU Direct RDMA不能使用IOMMU？**

https://mp.weixin.qq.com/s/O3r80GDeLj9aFtHU6sQ75Q

**摘要:** 文章通过解释GPU Direct RDMA与IOMMU的不兼容性和GPU在AI领域的使用原理，可以加深对GPU相关技术的理解。

**2.** **基于多光路交换机的可重配置数据中心吞吐量优化**

https://mp.weixin.qq.com/s/k7U_zz_SIX_MSJH6XPfluA

**摘要:** 本文提出的Leaf方案通过部署多个OCS，并以合作方式传输流量，有效提高了数据中心的吞吐量，并降低了运行时间。

## 大模型技术

**1.** **MLSys 2023 最佳论文 Efficiently Scaling Transformer Inference**

https://mp.weixin.qq.com/s/sMcFcOf6cOgJAO15wU1Afg

**摘要:** 研究通过先进的底层优化技术和创新的并行策略，显著提升了Transformer模型在大型深度模型、严格延迟限制和长序列长度下的推理效率和性能，为Transformer模型的高效推理提供了有效的解决方案。

**2.** **Intel CPU AI推理 学习笔记**

https://mp.weixin.qq.com/s/NYYoTKFhZ_uiWbBxKNtrtg

**摘要:** Intel CPU在AI推理领域具有显著优势，特别是第五代至强通过创新设计和制程技术改进，进一步提升了其推理性能。对于寻求高性价比解决方案的企业来说，CPU是一个值得考虑的选项。

## 技术之外

**1.** **构建10万张H100的大模型训练集群！**

https://mp.weixin.qq.com/s/mkWgHqOY9m-GAdAhUCBqbQ

**摘要:** 这篇文章总结了构建 10 万个 H100 集群所需要的技术, 包括电力能源, 网络拓扑和网络连接等维度, 这是一项庞大的系统工程.

**2.** **如何阅读一篇论文**

https://mp.weixin.qq.com/s/iOxaPvEVd0a6xRY6XcHF_w

**摘要:** 本文总结了阅读论文的三步法, “三步法”不仅提高了论文阅读效率，还帮助研究者更深入地理解论文内容。
