# 云计算技术动态 2024 年第十八期

## 分布式系统

**1.** **QLC SSD替换HDD，有什么问题？**

https://mp.weixin.qq.com/s/R3kzsMK6BFUpj6afqKMlpQ

**摘要:** 云端的算力提升，要求进一步复用资源，机械硬盘带宽固定且过低，容量扩大以后每TB的吞吐下降了无法保持SLO。高性能SSD空间有限也贵。本文希望部署QLC，但QLC的直接部署问题很大，本文尝试了简单方案，发现没法提供预期的性能。本文在阿里的ECS下设计了一个高性能盘+ZNS QLC盘的云本地硬盘方案，CSAL。CSAL已经在阿里云中上千个ECS节点进行了部署。

**2.** **利用RDMA技术提升Azure存储能力**

https://mp.weixin.qq.com/s/0_nILUB893qIpQIoAiJNtw

**摘要:** 本文介绍了在Azure中部署区域范围内（intra-region）RDMA以支持存储工作负载的经验。面对基础设施的高度复杂性和异构性所带来的新挑战，Azure中约70%的流量已通过RDMA传输，且区域内RDMA已在所有Azure公共区域得到全面支持。RDMA技术的引入，显著提升了硬盘I/O性能并减少了CPU核心占用。

## 云计算技术

**1.** **突破内存瓶颈：链式内存缓冲区（LMB）助力PCIe设备性能飞跃**

https://mp.weixin.qq.com/s/nKwHz4g0TnUOhxlNJkRwXg

**摘要:** 作者提出了一种统一的LMB框架，旨在解决PCIe设备上板载DRAM短缺的问题。该框架暴露了CXL内存扩展器的DRAM资源，为CXL和PCIe设备提供了一种统一的内存管理机制。在真实SSD中调查这一问题后，很明显作者的LMB解决方案有效地补充了板载DRAM并确保了高性能。

**2.** **通向AGI的钥匙：10万H100超级AI算力集群**

https://mp.weixin.qq.com/s/fxuSt06Hn7ux4NCtTXOGLA

**摘要:** 近日SemiAnalysis发布了一篇重磅深度报告，题为《100,000 H100 Clusters: Power, Network Topology, Ethernet vs InfiniBand, Reliability, Failures, Checkpointing》，这篇文章进行了部分翻译和解读, AI 军备竞赛再一次开启.

## 大模型技术

**1.** **以KVCache为中心，分离式推理架构，LLM吞吐能力飙升75%**

https://mp.weixin.qq.com/s/_3qUt6B5gmrm1YaWUVAuvw

**摘要:** 本文介绍了 Mooncake，这是一种以 KVCache 为核心的解耦架构，旨在高效地为 LLM 服务，特别是在处理长上下文和超负荷场景中。我们讨论了在最大化整体有效吞吐量的目标与满足与延迟SLO 要求之间平衡的必要性、挑战和设计选择。

**2.** **OSDI 24 Serverless LLM：性能提升200倍**

https://mp.weixin.qq.com/s/DoxSI5M-jcdlSg000VEIng

**摘要:** 这篇论文介绍了一个专为大语言模型设计的serverless推理系统。该系统巧妙利用GPU服务器上丰富的闲置的内存和存储资源（空间&带宽），显著减少了昂贵的远程检查点下载成本，实现了模型的高效加载。

## 技术之外

**1.** **对话AI原生丨大模型时代，智算基础设施将走向何方？**

https://mp.weixin.qq.com/s/qmKYuuE4qqMoV6QTEp1KhQ

**摘要:** 大模型时代对于智算基础设施提出了何种新要求？智算基础设施又将如何助力企业实现数智化转型？带着这些问题，在《对话AI原生：云智实验室》栏目中，百度集团产品委员会联席主席宋飞与InfoQ编辑围绕“大模型时代，智算基础设施如何实现超进化”展开了一场思想碰撞。

**2.** **CXL-GPU: 全球首款实现百ns以内的低延迟CXL解决方案**

https://mp.weixin.qq.com/s/qYoqtHDsZR8CxAnEpYm6NA

**摘要:** Panmnesia的CXL控制器通过独特的低延迟CXL IP解决了内存扩展中的延迟难题, 实现了两位数纳秒级往返延迟的CXL控制器, 给 GPU 提供了一个解决了GPU存储扩展中的设计方案。
