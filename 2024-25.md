# 云计算技术动态 2024 年第二十五期

## 分布式系统

**1.** **LLM训练的存储需求：训练数据和检查点**

https://mp.weixin.qq.com/s/1WG6VlISdeADJYMQ8fiY7w

**摘要:** 本文主要探讨了 LLM 训练中的存储需求和检查点相关问题。包括训练数据和检查点的存储特点、不同模型的并行化方式、检查点的操作与可恢复性、基础设施配置、数学分析以及误区澄清等方面。

**2.** **大规模分布式 AI 模型训练系列——张量并行**

https://mp.weixin.qq.com/s/V8SQWA9O8i5fPJF3etbxPA

**摘要:** 本文详细介绍了分布式训练中的张量并行（Tensor Parallelism，TP），包括多种方案及应用场景。涵盖了从 AlexNet 到现代的 Colossal--AI、Megatron--LM 等技术，还讨论了在大规模分布式训练和 LLM 推理中的应用及挑战。

## 云计算技术

**1.** **HotChips2024，Tesla TTPoE**

https://mp.weixin.qq.com/s/iCGo5n0T4qZVk2fxFtNoPg

**摘要:** 在 HC2024 上，Tesla 带来自研以太网传输协议 TTPoE 的分享。Tesla 未采用典型网络解决方案，而是通过修改传输层使用以太网特性满足 AI 训练需求。Tesla 自研协议具有低延迟、高带宽、实现简单等特性，完全硬件实现，采用有损传输和简单拥塞控制，状态机简单，通过硬件处理连接等操作。

**2.** **HotChip2024后记: 谈谈加速器互联及ScaleUP为什么不能用RDMA**

https://mp.weixin.qq.com/s/qLRC3dv4E93LwWXtuhQcsw

**摘要:** 本文主要探讨了 AI 加速器的互联系统，特别是关于 HotChip 2024 介绍的相关内容，重点分析了 RDMA 在 ScaleUP 网络中的应用及存在的问题，同时探讨了适合 ScaleUP 网络的协议。

## 大模型技术

**1.** **Transparent GPU Sharing in Container Clouds for Deep Learning Workloads（NSDI’23）**

https://mp.weixin.qq.com/s/bO0Hn1GhoMM5uwT9I-3H-g

**摘要:** TGS（Transparent GPU Sharing）是一个系统，它为容器云中的深度学习训练提供了透明的GPU共享。与应用层的GPU共享解决方案，TGS在容器底层的操作系统层运行，同时优化系统层Nvidia MPS和MIG性能和异常隔离、GPU过订阅等问题，允许用户在容器中使用任何软件来开发和运行模型。

**2.** **基于CXL持久内存分解的容错训练**

https://mp.weixin.qq.com/s/o2p3LOgVNbiqQ1br6BRlyg

**摘要:** 这篇文章主要介绍了一种名为 TRAININ--CXL 的技术，用于处理大规模推荐数据集的深度学习训练，具有容错性且能提高训练性能和节能。

## 技术之外

**1.** **亚马逊AWS CEO：算力资源会持续紧张，AI需求几乎无穷无尽**

https://mp.weixin.qq.com/s/Q9MILEpiUbRc8cs1iq2g6A

**摘要:** AWS CEO Matt Garman 接受《No Priors》播客节目的采访，谈论了 AWS 早期起源、发展策略、云计算和 AI 的发展趋势，以及行业面临的算力资源短缺等话题。在 40 多分钟的采访中，Matt Garman讲述了在 AI 时代，AWS 的发展策略。从他的表述上看，他对于 AWS 在 AI 时代获得进一步增长满怀信心。

**2.**  **探索CXL的应用场景和实现**

https://mp.weixin.qq.com/s/tFaJuaPRbC4zv9SrfxiuHw

**摘要:** 本次演讲主要探讨了 CXL 技术的相关内容，包括内存墙问题及解决方案、CXL 技术的应用与发展、光学技术在 CXL 中的应用等。
