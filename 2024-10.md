# 云计算技术动态 2024 年第十期

## 分布式系统

**1.** **KubeCon Europe 2024 ｜ AI 时代的云原生共识详解**

https://mp.weixin.qq.com/s/11fC_tBq45xMDu2f2InIIg

**摘要:** 本文介绍了在AI时代，业界对云原生技术的共识，特别是GPU利用率和最佳实践的关注。文章提到了云原生AI的白皮书和全景图，并重点讨论了调度器、GPU共享、DRA、Kueue等技术。

**2.** **深入理解 Linux 异步 I/O 框架 io_uring**

https://mp.weixin.qq.com/s/JtgjILggKbsVyt7xgG_a6w

**摘要:** 本文介绍了Linux异步I/O框架io_uring，它是2019年Linux 5.1内核引入的高性能异步I/O框架，能显著加速I/O密集型应用的性能。与传统的Linux AIO相比，io_uring具有更好的灵活性和可扩展性，支持更多的异步系统调用。虽然性能提升有限，但io_uring统一了Linux异步I/O框架，是革命性的技术。

## 云计算技术

**1.** **浅谈 CCD/CCX 与 NUMA**

https://mp.weixin.qq.com/s/8vM4w2tmnHIhdgl_Cylkxw

**摘要:** 本文主要探讨了CPU的基本构造和NUMA的划分，特别关注了CCD/CCX与NUMA的关系。文章详细介绍了CPU的不同设计，包括单Die、多Die等，并深入探讨了MCM架构下的NUMA划分。文章还结合了AMD和Intel处理器的实例，帮助读者更好地理解NUMA的划分。

**2.** **RDMA这十年的反思1：从协议演进的视角**

https://mp.weixin.qq.com/s/4yHS0MTXwTsUKjVryJvItg

**摘要:** RDMA技术自1995年诞生至今，经历了多次演进和挑战。RoCEv2在拥塞控制方面误入歧途，导致问题长期困扰工业界。相比之下，AWS和Google等公司的SRD和Falcon等现代化解决方案更为清晰和有效。AI集群需要高带宽、低延迟，避免长尾问题的网络设计。拥塞控制应简洁有效，避免复杂依赖。网络设计需遵循"Smart Edge, Dumb Core"原则，避免复杂的相互依赖解决方案。

## 大模型技术

**1.** **大模型推理框架vLLM源码解析**

https://mp.weixin.qq.com/s/TjrJKzRcbwZxtA0aundfRA

**摘要:** 本文介绍了vLLM源码的架构和调用方式，包括Offline Batched Inference和Online Serving两种方法。文章首先概述了vLLM的代码结构，然后详细介绍了Offline Batched Inference的API和LLMEngine内核引擎的实现细节。

**2.** **神经网络背后的数学**

https://mp.weixin.qq.com/s/CVoz6zuTzAI6Htgcy478fA

https://mp.weixin.qq.com/s/pnYLkG4xRyjqsba_QpWD5g

**摘要:** 本文介绍了神经网络的基本知识，包括神经网络的由来、层类型、激活函数和梯度概念。文章通过一个形象的例子阐述了神经网络的基本概念，并解释了权重、偏置和激活函数的作用。此外，还介绍了激活函数的发展和多种函数的应用，以及梯度在神经网络中的重要性。

## 技术之外

**1.** **GB200的性能成本定量分析**

https://mp.weixin.qq.com/s/oy5yh1gh2c9d7eK85aNwmA

**摘要:** 本文对Nvidia发布的B100、B200和GB200等新一代GPU进行了性能成本定量分析。文章通过自己开发的LLM模型性能模拟器，分析了各种应用的实际性能提升，并探讨了竞争对手和云厂自制ASIC的竞争力。文章还讨论了不同工作负载、模型规模和参数对性能的影响，以及GPU持续有效性的重要性。

**2.** **为什么说“互联”会是未来科技投资重要命题**

https://mp.weixin.qq.com/s/ddzEW0mF74XCpup7zmWY6Q

**摘要:** 本文探讨了“互联”在科技投资中的重要性，并从微软和OpenAI的合作、台积电的GPU晶体管数量增长等方面进行了分析。文章指出，随着应用规模急速膨胀，现有技术单元互联成为必要，而“scaling law”正在创造出一个巨大计算系统，需要持续迭代加速互联技术。

