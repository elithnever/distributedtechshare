# 云原生技术动态 2023 年第十九期

## 分布式系统

**1.** **GPTCache：通过缓存LLM查询成本降低 10 倍，速度提高 100 倍**

https://mp.weixin.qq.com/s/zfKlk4F_959RUcaJruDptw

**摘要:** GPTCache 是 LLM 语义缓存层（caching layer），它采用语义缓存（*semantic cache*）技术，能够存储 LLM 响应，GPTCache允许用户根据需要自定义缓存，包括嵌入函数，相似性评估函数，存储位置和逐出的选项。

**2.** **揭秘GPT-4：OpenAI在架构设计中所做的工程权衡**

https://mp.weixin.qq.com/s/AIwinPksV_u-RQfcoCD7nw

**摘要:** 这是一篇今天网络发布的一篇GPT-4内部技术解密文档，原文：GPT-4 Architecture, Infrastructure, Training Dataset, Costs,Vision, MoE （GPT-4架构、基础设施、训练数据集、成本、视觉和MoE ） by Dylan Patel and Gerald Wong 2023.7.11.

## 云计算技术

**1.** **深入理解SPDK读写数据的过程，从应用到NVMe驱动**

https://mp.weixin.qq.com/s/ua2JTutmiJkC0upVE54z5A

**摘要:** 本文将深入从NVMe驱动的实现层面介绍一下数据读写的相关内容。在SPDK的NVMe驱动中，两个基本的读写API分别是spdk_nvme_ns_cmd_read和spdk_nvme_ns_cmd_write。这两个API提供基于缓冲区的数据读写，也就是数据是在一段连续的内存缓冲区中。

**2.** **英特尔DSA-加速DPDK Vhost** 

https://mp.weixin.qq.com/s/O_S9v6DGoc4HmamkJyuKEA

**摘要:** 这篇文章概述了采用Intel DSA加速的DPDK Vhost的设计以及DPDK Vhost异步API的使用方法，并在第四代英特尔至强可扩展处理器上测试了带有Intel DSA加速的TestPMD数据包转发速率。在数据包大小超过256字节时，将数据包拷贝卸载到Intel DSA可以显著提高数据包的转发性能。

## 云开发技术

**1.** **新的网络性能优化技术 —— BIG TCP**

https://mp.weixin.qq.com/s/0bQlcxRzBMaxTxOC4c8pmg

**摘要:** 在 100G 网络时代, 传统以太网 MTU 只有 1500 字节, 单核 cpu 需要做分包等计算逻辑, 在网卡没有打满的情况下, cpu 已经显现瓶颈了, 这篇文章介绍的 big tcp 技术就是在内核中支持更大的分片来提升网络性能.

**2.** **向量检索在大模型应用场景的技术和实践**

https://mp.weixin.qq.com/s/-io_q8WCdAxfSCY9qlrgCw

**摘要:** 随着大语言模型的应用越来越广泛, 向量检索技术再次映入大家的视野, 这篇文章介绍了向量检索技术的核心原理, 同时也分享了百度云大规模向量检索技术的实践. 

## 技术之外

**1.** **经纬张颖：AI的远与近**

https://mp.weixin.qq.com/s/-nlpjedR89VBxi-rQtGVWA

**摘要:** 无论是ToC或ToB的公司，打造反馈闭环、形成数据飞轮也是理所当然的一个选择。而对一些从零开始的小公司，这件事可能会很难，但也因为你们没有包袱，所以也有机会。

**2.** **ChatGPT|万字长文总结吴恩达prompt-engineering课**

https://mp.weixin.qq.com/s/gUtB71uWI7Dg_tfRzaidCA

**摘要:** 吴恩达《ChatGPT Prompt Engineering for Developers》课程中文版，主要内容为指导开发者如何构建 Prompt 和 利用 OpenAI API 构建新的 LLM 的应用.
