# 云计算技术动态 2024 年第十二期

## 分布式系统

**1.** **大规模弹性部署：Google如何管理TPUv4集群**

https://mp.weixin.qq.com/s/-ICg4ihulkTCHz0UB-TfDQ

**摘要:** 本文介绍了Google在TPUv4集群中的弹性部署和可靠性运维实践，包括使用SDN管理TPUv4之间的互联、OCS绕过故障节点、自动重配置和故障绕行等技术，实现了99.98%的系统可用性，并优雅地处理了硬件中断。文章还提到了Google TPU v4的软硬件组成和集群管理调度软件。

**2.** **生成式AI的GPU网络**

https://mp.weixin.qq.com/s/8gO_RdJfyqzL8WcycoyFwg

https://mp.weixin.qq.com/s/5H8JJvlGdsvpURsDVEVywg

**摘要:** 本文介绍了生成式AI的GPU网络，特别是GPU集群的分区并行张量并行。矩阵乘法在多个GPU上并行化，每个GPU负责部分乘法并广播输入X。张量并行GPU之间需要all-to-all通信，通信大小取决于微批次大小和隐藏层大小。在连续流水线阶段之间，需要分散-聚集方案来减少数据通信量。梯度聚合使用双二叉树机制提供全带宽和对数延迟。

## 云计算技术

**1.** **GPU计算的工作原理**

https://mp.weixin.qq.com/s/ArOFCAamLL31tkasOqd-Eg

**摘要:** 本文介绍了GPU计算的工作原理，包括FLOPS的重要性、内存带宽的影响、延迟的关键性、GPU的吞吐量、线程层次结构以及数据位置对计算效率的影响。

**2.** **使用 DPDK 和 GPUdev 在 GPUs上增强内联数据包处理**

https://mp.weixin.qq.com/s/WdncTLxTKjPQuqq5zcuqoA

**摘要:** 本文介绍了如何使用DPDK和GPUdev在GPU上增强内联数据包处理。主要涉及优化网络控制器和GPU之间的数据交换，以及CPU协调GPU和网卡的工作流程。文章还介绍了使用GPUDirect RDMA技术实现数据流优化，以及控制流的四种不同方法。

## 大模型技术

**1.** **Google Cloud AI平台及其基础设施**

https://mp.weixin.qq.com/s/CZiqA-7fqA30udhNfknWiQ

**摘要:** 该文介绍了Google Cloud AI平台及其基础设施，包括AI模型的发展、开放模型生态系统及其未来发展，以及Google Cloud如何助力这些模型的构建和发展。文章还重点介绍了Google Cloud的AI基础设施和平台如何接纳和部署开放模型，以及如何将开放模型融入生成式AI战略中。

**2.** **LLM 推理性能工程最佳实践**

https://mp.weixin.qq.com/s/5YSiKcucwv1FtiKOH1y0gw

**摘要:** 本文介绍了如何衡量LLM推理性能的四个关键指标，并探讨了优化LLM推理的通用技术和特定于Transformer的优化方法，如算子融合、量化、并行化等。文章还提到了共享推理服务的成本效率和模型带宽利用率的重要性。

## 技术之外

**1.** **算力受限下的大模型发展和AI基础设施建设**

https://mp.weixin.qq.com/s/B2aEEwEEvOaQ8XOpQX5j2g

**摘要:** 本文探讨了在算力受限下的大模型发展和AI基础设施建设的问题。文章从多个角度进行了分析，包括商业模式、算法、算力芯片架构和互联等，并提出了系统性的解决方案。文章还从商业逻辑、算法层面和算力芯片的调度和编排能力等方面进行了深入探讨，并强调了云计算的弹性和多租的重要性。

**2.** **IPU / DPU 双雄对决：英特尔 vs 英伟达**

https://mp.weixin.qq.com/s/ZauOISSt5YyrWP8oryBZXQ

**摘要:** 本文对IPU/DPU市场的两大领导者英特尔和英伟达的新产品IPU E2000和BlueField-3进行了比较。文章介绍了产品的发布情况、技术细节和内部设计，并探讨了双方产品的优缺点。
