# 云计算技术动态 2024 年第十一期

## 分布式系统

**1.** **Google：使用Cloud Storage统一分析工作负载数据**

https://mp.weixin.qq.com/s/ADCr6ZwPnK-dqwxZIMGZeA

**摘要:** 本文介绍了Google Cloud如何使用Cloud Storage统一分析工作负载数据，以解决数据孤岛问题。文章强调了Cloud Storage的可靠性和经济高效性，并探讨了如何支持多种类型的工作负载和用户类型。文章还提到了性能、治理和可编程性等方面的重要性。

**2.** **Dynamic Resource Allocation (DRA) 的重大变化**

https://mp.weixin.qq.com/s/v9PXmlEE0at1mjYNWO7mGw

**摘要:** 本文介绍了Dynamic Resource Allocation (DRA)的重大变化，包括其面临的问题、架构变化和未来规划。文章首先回顾了DRA的整体架构和调度流程，然后分析了调度问题的根本原因，最后探讨了新架构如何解决性能问题。

## 云计算技术

**1.** **谈谈基于以太网的GPU Scale-UP网络**

https://mp.weixin.qq.com/s/HANcLaN5meygpt4RL5YbFA

**摘要:** 本文探讨了以太网替代NVLink在GPU Scale-UP网络中的实现问题。文章列举了需要解决的问题，包括延迟边界、传输语义、大内存池化以及动态路由和拥塞控制能力。文章还介绍了Intel Gaudi3和Microsoft Maia100的Scale-UP互联方案。

**2.** **KubeCon EU 后记之模型推理**

https://mp.weixin.qq.com/s/LbFppBCdonLbRZZauwCpgQ

**摘要:** 本文讨论了在KubeCon EU上关于模型推理服务的几个关键点，包括模型加载、模型管理和弹性伸缩。文章介绍了在Kubernetes平台上部署模型推理服务时遇到的问题和解决方案，并涉及了一些开源项目。

## 大模型技术

**1.** **全网首发，Meta Llama-3 全方位详解**

https://mp.weixin.qq.com/s/xtv0C730DP-DQq72uLUJ3A

**摘要:** Meta Llama-3 是一个开源的语言模型，具有卓越的性能和广泛的应用场景。它采用了优化的自回归 Transformer 架构，结合了监督式微调和带人类反馈的强化学习，确保了模型的安全性和可靠性。Llama-3 支持多种商业和研究用途，已在多个行业标准测试中展示了其卓越的性能。

**2.** **MoE模型的前世今生**

https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag

**摘要:** 本文介绍了MoE模型的发展历程，包括上古时代、RNN时代和Transformer时代的重要工作。文中还提到了近期发布的MoE模型，并推荐了DeepSeek-MoE和Qwen1.5-MoE两个中文领域做得比较好的工作。

## 技术之外

**1.** **Meta超大规模AI/GPU基础设施架构2024**

https://mp.weixin.qq.com/s/NJUkRYV6J5-IKbGUfKplqQ

**摘要:** Meta宣布推出大规模AI集群，每个集群由2.4w张GPU组成，用于Llama 3训练。集群设计细节包括计算、网络、存储、设计、性能和软件等方面，旨在为各种AI工作负载提取高吞吐量和可靠性。Meta致力于开放计算和开源，并发布基础设施路线图，目标是扩大基础设施建设，包括350,000个NVIDIA H100 GPU。这些集群支持当前和下一代的人工智能模型，包括Llama3、Llama2的继任者等。

**2.** **互连成为异构集成架构的重要组件**

https://mp.weixin.qq.com/s/YAc6yCbGCipmGg1IqHF2-g

**摘要:** 本文介绍了互连在异构集成架构中的重要性，随着芯片行业的发展，设计和制造互连变得越来越复杂，对设备可靠性也越来越重要。文章讨论了如何克服物理限制、提高速度、节省功耗，以及互连的未来发展方向。

